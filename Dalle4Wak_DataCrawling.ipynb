{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cksdlakstp12/chegyedan-computational-hanmadang/blob/main/Dalle4Wak_DataCrawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium==3.141"
      ],
      "metadata": {
        "id": "NTaeZChIJ95z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0028c74a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from datetime import date\n",
        "import time\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from threading import Thread\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options \n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import urllib.request\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr00EkN3JQF3"
      },
      "outputs": [],
      "source": [
        "def str2float(text):\n",
        "  if \"만\" in text:\n",
        "    text = text.replace(\"만\", \"\").replace(\",\", \"\")\n",
        "    result = float(text) * 10000\n",
        "      \n",
        "  elif \"천\" in text:\n",
        "    text = text.replace(\"천\", \"\").replace(\",\", \"\")\n",
        "    result = float(text) * 1000\n",
        "\n",
        "  else:\n",
        "    text = text.replace(\",\", \"\")\n",
        "    if not text.isnumeric():\n",
        "        text = 0\n",
        "    result = float(text)\n",
        "      \n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsRiU0i5G8kX"
      },
      "outputs": [],
      "source": [
        "class NoHeadDriver():\n",
        "  def __init__(self, css_selectors, name_to_url):\n",
        "    self.driver = self.init_driver()\n",
        "\n",
        "    self.css_selectors = css_selectors\n",
        "    self.name_to_url = name_to_url\n",
        "\n",
        "  def init_driver(self):\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    prefs = {\"profile.default._setting_values.notifications\":1}\n",
        "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "    driver = webdriver.Chrome(\"chromedriver\", chrome_options=chrome_options)\n",
        "    driver = webdriver.Chrome(\"chromedriver\")\n",
        "    return driver\n",
        "\n",
        "  def scrolling_to_bottom(self):\n",
        "    print(\"Start scrolling...\", end=\"\\t\")\n",
        "    scroll_start_time = time.time()\n",
        "    before_h = self.driver.execute_script(\"return window.scrollY\")\n",
        "    while True:\n",
        "      self.driver.find_element_by_css_selector('body').send_keys(Keys.END) #맨 아래로 스크롤 내림\n",
        "      time.sleep(1) #스크롤 사이 페이지 로딩 시간\n",
        "      after_h = self.driver.execute_script(\"return window.scrollY\")\n",
        "      if after_h == before_h:\n",
        "        break\n",
        "      before_h = after_h\n",
        "    print(\"Scrolling end in :\", time.time() - scroll_start_time)\n",
        "  \n",
        "  def move_to_page(self, page_url):\n",
        "    self.driver.get(page_url)\n",
        "    self.driver.implicitly_wait(1)    \n",
        "    if \"iframe\" in self.css_selectors: # if not switch to iframe, will must occur NoSuchElementException\n",
        "      self.driver.switch_to.frame(self.driver.find_element_by_css_selector(self.css_selectors[\"iframe\"]))\n",
        "\n",
        "  def file_compressing(self, dir):\n",
        "    print(f\"Start {dir} files compressing...\", end=\"\")\n",
        "    download_start_time = time.time()  \n",
        "    with zipfile.ZipFile(f'{dir.split(\"/\")[-1]}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as new_zip:\n",
        "\n",
        "      ROOT_PATH = f\"{dir}/\"\n",
        "      file_name_list = os.listdir(ROOT_PATH)\n",
        "\n",
        "      for file_name in file_name_list:\n",
        "        new_zip.write(os.path.join(ROOT_PATH, file_name), arcname=file_name) \n",
        "  \n",
        "    print(f\" : done in {time.time() - download_start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ86CdlGf_6w"
      },
      "outputs": [],
      "source": [
        "class WakZooFanArtCrawler(NoHeadDriver):\n",
        "  def __init__(self, channel_name_to_url, visit_thres=0, likes_thres=0):\n",
        "    css_selectors = {\n",
        "        \"date\":\"#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_date\",\n",
        "        \"title\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_article > div.board-list > div > a.article',\n",
        "        \"article_inner_title\":\"#app > div > div > div.ArticleContentBox > div.article_header > div.ArticleTitle > div > h3\",\n",
        "        \"visit\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_view',\n",
        "        \"likes\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_likes',\n",
        "        \"image\":\".se-image-resource\",\n",
        "        \"iframe\":'#cafe_main',\n",
        "        \"page_bar\":\"#main-area > div.prev-next\"\n",
        "    }\n",
        "    super().__init__(css_selectors, channel_name_to_url)\n",
        "    self.visit_thres = visit_thres\n",
        "    self.likes_thres = likes_thres\n",
        "\n",
        "  def run_crawler(self, dir=None, only_today=False):\n",
        "    thread_list = []\n",
        "    for channel_name, channel_url in self.name_to_url.items():\n",
        "      if dir is not None:\n",
        "        channel_name = dir\n",
        "\n",
        "      os.makedirs(channel_name, exist_ok=True)\n",
        "      self.crawling(channel_name, channel_url, only_today)\n",
        "      self.file_compressing(channel_name)\n",
        "\n",
        "      print(\"number of files : \", len(os.listdir(f\"{channel_name}\")))\n",
        "      print(\"=\"*50, end=\"\\n\\n\")\n",
        "\n",
        "    self.driver.close()\n",
        "\n",
        "  def download_image_and_caption(self, folder_name, file_name):\n",
        "    images = self.driver.find_elements_by_css_selector(self.css_selectors[\"image\"])\n",
        "    for img_idx, img in enumerate(images):\n",
        "      time.sleep(1)\n",
        "      file_full_name = f\"{file_name}_img{img_idx}\"\n",
        "      try:\n",
        "        with open(f\"{folder_name}/{file_full_name}.txt\", \"w\") as f:\n",
        "          title_text = self.driver.find_element_by_css_selector(self.css_selectors[\"article_inner_title\"]).text\n",
        "          f.write(title_text)\n",
        "\n",
        "        imgUrl = img.get_attribute(\"src\")\n",
        "        urllib.request.urlretrieve(imgUrl, f\"{folder_name}/{file_full_name}.jpg\")\n",
        "      except:\n",
        "        for f in glob.glob(f\"{folder_name}/{file_full_name}.*\"):\n",
        "          os.remove(f)\n",
        "\n",
        "  def crawling(self, channel_name, page_url, only_today):\n",
        "    print(f\"Start {channel_name} image crawling...\", end=\"\") \n",
        "    start_time = time.time()     \n",
        "    for page_idx in range(1, 1000 + 1):\n",
        "      current_url = page_url.replace(\"changable_index\", str(page_idx))\n",
        "      self.move_to_page(current_url) # 크롤링하고자 하는 페이지로 들어감\n",
        "      \n",
        "      for table_idx in range(1, 16): # 팬아트 게시물은 15개씩 table로 나온다.\n",
        "        # 기본 정보 추출\n",
        "        art_date = self.driver.find_element_by_css_selector(self.css_selectors[\"date\"].replace(\"changable_index\", str(table_idx)))\n",
        "        title = self.driver.find_element_by_css_selector(self.css_selectors[\"title\"].replace(\"changable_index\", str(table_idx)))\n",
        "        visit = str2float(self.driver.find_element_by_css_selector(self.css_selectors[\"visit\"].replace(\"changable_index\", str(table_idx))).text)\n",
        "        likes = str2float(self.driver.find_element_by_css_selector(self.css_selectors[\"likes\"].replace(\"changable_index\", str(table_idx))).text)\n",
        "        \n",
        "        if only_today and len(art_date.text) > 6:\n",
        "          break\n",
        "          \n",
        "        if len(art_date.text) <= 6:\n",
        "          art_date = date.today().isoformat()\n",
        "        \n",
        "        # 조회수와 좋아요 수로 필터링\n",
        "        if visit < self.visit_thres or likes < self.likes_thres:\n",
        "          continue\n",
        "\n",
        "        self.move_to_page(title.get_attribute('href')) # 팬아트 게시물 내부로 들어간다.\n",
        "        self.download_image_and_caption(folder_name=channel_name, file_name=f\"{art_date}{page_idx}\") # 이미지를 다운로드 한다.\n",
        "        self.move_to_page(current_url) # 뒤로가기\n",
        "          \n",
        "      else: \n",
        "        continue\n",
        "          \n",
        "      break\n",
        "    \n",
        "    print(f\" : done in {time.time() - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWzQ2vdqW24W"
      },
      "outputs": [],
      "source": [
        "class YouTubeThumbNailCrawler(NoHeadDriver):\n",
        "  def __init__(self, channel_name_to_url):\n",
        "    css_selectors = {\n",
        "        \"thumbnail\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/ytd-thumbnail/a\",\n",
        "        \"title\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/div[1]/div[1]/h3/a\"\n",
        "    }\n",
        "    super().__init__(css_selectors, channel_name_to_url)\n",
        "\n",
        "  def run_crawler(self, dir=None):\n",
        "    for channel_name, channel_url in self.name_to_url.items():\n",
        "      if dir is not None:\n",
        "        channel_name = dir\n",
        "\n",
        "      print(f\"Start {channel_name} image crawling...\")\n",
        "      start_time = time.time()\n",
        "\n",
        "      os.makedirs(channel_name, exist_ok=True)\n",
        "      self.move_to_page(channel_url)\n",
        "      self.scrolling_to_bottom()\n",
        "      self.crawling(channel_name)\n",
        "      self.file_compressing(channel_name)\n",
        "\n",
        "      print(f\" : done in {time.time() - start_time}\")\n",
        "      print(\"number of files : \", len(os.listdir(f\"{channel_name}\")))\n",
        "      print(\"=\"*50, end=\"\\n\\n\")\n",
        "\n",
        "    self.driver.close()\n",
        "\n",
        "  def crawling(self, channel_name):  \n",
        "    idx = 0\n",
        "    while True:\n",
        "      idx += 1\n",
        "      title_xpath = self.css_selectors[\"title\"]\n",
        "      image_xpath = self.css_selectors[\"thumbnail\"]\n",
        "      try:\n",
        "        title = self.driver.find_element_by_xpath(title_xpath.replace(\"changable_index\", str(idx))).text\n",
        "        if \"#Shorts\" in title or \"#shorts\" in title: \n",
        "          continue\n",
        "        image = self.driver.find_element_by_xpath(image_xpath.replace(\"changable_index\", str(idx)))\n",
        "        \n",
        "        file_name = date.today().isoformat()\n",
        "        with open(f\"{channel_name}/{file_name}_img{idx}.txt\", \"w\") as f:\n",
        "          f.write(title)\n",
        "            \n",
        "        title = self.driver.find_element_by_xpath(title_xpath.replace(\"changable_index\", str(idx))).text\n",
        "        if \"#Shorts\" in title or \"#shorts\" in title: \n",
        "          continue\n",
        "          \n",
        "        image = self.driver.find_element_by_xpath(image_xpath.replace(\"changable_index\", str(idx)))\n",
        "        time.sleep(2)\n",
        "        image_url = \"https://i.ytimg.com/vi/\"+image.get_attribute(\"href\").replace(\"https://www.youtube.com/watch?v=\",\"\")+\"/hqdefault.jpg\"      \n",
        "        urllib.request.urlretrieve(image_url, f\"{channel_name}/{file_name}_img{idx}.jpg\")\n",
        "          \n",
        "      except NoSuchElementException:\n",
        "        break\n",
        "          \n",
        "      except:\n",
        "        for f in glob.glob(f\"{channel_name}/{file_name}_img{idx}.*\"):\n",
        "          os.remove(f)\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9393d5e6"
      },
      "outputs": [],
      "source": [
        "visit_thres = 1000\n",
        "likes_thres = 100\n",
        "\n",
        "wakzoo_name_to_url = {\n",
        "    \"isedol_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=344%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"gomem_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=299%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"hyung_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=59%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"geumson\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=551%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "}\n",
        "\n",
        "youtube_name_to_url = {\n",
        "    \"gosegu_youtube\":\"https://www.youtube.com/channel/UCV9WL7sW6_KjanYkUUaIDfQ/videos\",\n",
        "    \"lilpa_youtube\":\"https://www.youtube.com/channel/UC-oCJP9t47v7-DmsnmXV38Q/videos\",\n",
        "    \"viichan_youtube\":\"https://www.youtube.com/channel/UCs6EwgxKLY9GG4QNUrP5hoQ/videos\",\n",
        "    \"ine_youtube\":\"https://www.youtube.com/channel/UCroM00J2ahCN6k-0-oAiDxg/videos\",\n",
        "    \"jururu_youtube\":\"https://www.youtube.com/c/%EC%A3%BC%EB%A5%B4%EB%A5%B4/videos\",\n",
        "    \"jingberger_youtube\":\"https://www.youtube.com/c/%EC%A7%95%EB%B2%84%EA%B1%B0/videos\",\n",
        "    \"wakgoob_youtube\":\"https://www.youtube.com/user/woowakgood/videos\",\n",
        "    \"waktaverse\":\"https://www.youtube.com/c/welshcorgimessi/videos\",\n",
        "    \"gyeleug\":\"https://www.youtube.com/channel/UChCqDNXQddSr0ncjs_78duA/videos\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhM520GjW29G"
      },
      "outputs": [],
      "source": [
        "fan_art_crawler = WakZooFanArtCrawler(wakzoo_name_to_url, visit_thres, likes_thres)\n",
        "fan_art_crawler.run_crawler()\n",
        "\n",
        "youtube_thumbnail_crawler = YouTubeThumbNailCrawler(youtube_name_to_url)\n",
        "youtube_thumbnail_crawler.run_crawler()\n",
        "\n",
        "for name in wakzoo_name_to_url.keys():\n",
        "  os.system(f\"cp /content/{name}.zip /content/drive/MyDrive/dataset/Dalle4Wak\")\n",
        "  print(f\"cp /content/{name}.zip /content/drive/MyDrive/dataset/Dalle4Wak\")\n",
        "for name in youtube_name_to_url.keys():\n",
        "  os.system(f\"cp /content/{name}.zip /content/drive/MyDrive/dataset/Dalle4Wak\")\n",
        "  print(f\"cp /content/{name}.zip /content/drive/MyDrive/dataset/Dalle4Wak\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hbk8EMHJQF_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Dalle4Wak_DataCrawling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}