{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cksdlakstp12/chegyedan-computational-hanmadang/blob/main/Dalle4Wak_DataCrawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0028c74a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "from threading import Thread\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options \n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import urllib.request\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5wB5pNGCYW0"
      },
      "outputs": [],
      "source": [
        "def str2float(text):\n",
        "    if \"만\" in text:\n",
        "        text = text.replace(\"만\", \"\").replace(\",\", \"\")\n",
        "        result = float(text) * 10000\n",
        "        \n",
        "    elif \"천\" in text:\n",
        "        text = text.replace(\"천\", \"\").replace(\",\", \"\")\n",
        "        result = float(text) * 1000\n",
        "    \n",
        "    else:\n",
        "        text = text.replace(\",\", \"\")\n",
        "        if not text.isnumeric():\n",
        "            text = 0\n",
        "        result = float(text)\n",
        "        \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsRiU0i5G8kX"
      },
      "outputs": [],
      "source": [
        "class NoHeadDriver():\n",
        "    def __init__(self, css_selectors, name_to_url):\n",
        "        self.driver = self.init_driver()\n",
        "\n",
        "        self.css_selectors = css_selectors\n",
        "        self.name_to_url = name_to_url\n",
        "\n",
        "    def init_driver(self):\n",
        "        chrome_options = webdriver.ChromeOptions()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        prefs = {\"profile.default._setting_values.notifications\":1}\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "        driver = webdriver.Chrome(\"chromedriver\", chrome_options=chrome_options)\n",
        "        driver = webdriver.Chrome(\"chromedriver\")\n",
        "        return driver\n",
        "\n",
        "    def scrolling_to_bottom(self):\n",
        "        print(\"Start scrolling...\", end=\"\\t\")\n",
        "        scroll_start_time = time.time()\n",
        "        before_h = self.driver.execute_script(\"return window.scrollY\")\n",
        "        while True:\n",
        "            self.driver.find_element_by_css_selector('body').send_keys(Keys.END) #맨 아래로 스크롤 내림\n",
        "            time.sleep(1) #스크롤 사이 페이지 로딩 시간\n",
        "            after_h = self.driver.execute_script(\"return window.scrollY\")\n",
        "            if after_h == before_h:\n",
        "                break\n",
        "            before_h = after_h\n",
        "        print(\"Scrolling end in :\", time.time() - scroll_start_time)\n",
        "    \n",
        "    def move_to_page(self, page_url):\n",
        "        self.driver.get(page_url)\n",
        "        self.driver.implicitly_wait(1)    \n",
        "        if \"iframe\" in self.css_selectors: # if not switch to iframe, will must occur NoSuchElementException\n",
        "            self.driver.switch_to.frame(self.driver.find_element_by_css_selector(self.css_selectors[\"iframe\"]))\n",
        "\n",
        "    def file_compressing(self, dir):\n",
        "        print(f\"Start {dir} files compressing...\", end=\"\")\n",
        "        download_start_time = time.time()  \n",
        "        with zipfile.ZipFile(f'{dir.split(\"/\")[-1]}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as new_zip:\n",
        "\n",
        "            ROOT_PATH = f\"{dir}/\"\n",
        "            file_name_list = os.listdir(ROOT_PATH)\n",
        "\n",
        "            for file_name in file_name_list:\n",
        "                new_zip.write(os.path.join(ROOT_PATH, file_name), arcname=file_name) \n",
        "        \n",
        "        print(f\" : done in {time.time() - download_start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ86CdlGf_6w"
      },
      "outputs": [],
      "source": [
        "class WakZooFanArtCrawler(NoHeadDriver):\n",
        "    def __init__(self, channel_name_to_url, crawl_date_diff=None, visit_thres=0, likes_thres=0):\n",
        "        '''\n",
        "        channel_name_to_url -> dict\n",
        "            - url의 닉네임을 key로, url을 value로 하여 딕셔너리를 만들면 된다.\n",
        "            ex)    {\"gomem_fan_art\":\"https://www.-----\", ---}\n",
        "        \n",
        "        crawl_date_diff -> int\n",
        "            - 오늘로부터 몇 일까지 크롤링할지 지정\n",
        "            - default로 둘 시, 모든 게시물을 크롤링한다.\n",
        "            ex) 3으로 지정시 오늘로부터 3일전의 영상까지 크롤링한다.\n",
        "        \n",
        "        visit_thres, likes_thres -> int\n",
        "            - 이 파라미터 이상의 값인 게시물만 크롤링한다.\n",
        "            \n",
        "        method run_crawler -> void\n",
        "            - dir 파라미터에 경로를 주면 해당 경로에 데이터를 저장한다.\n",
        "        \n",
        "        '''\n",
        "        css_selectors = {\n",
        "            \"date\":\"#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_date\",\n",
        "            \"title\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_article > div.board-list > div > a.article',\n",
        "            \"article_inner_title\":\"#app > div > div > div.ArticleContentBox > div.article_header > div.ArticleTitle > div > h3\",\n",
        "            \"visit\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_view',\n",
        "            \"likes\":'#main-area > div:nth-child(4) > table > tbody > tr:nth-child(changable_index) > td.td_likes',\n",
        "            \"image\":\".se-image-resource\",\n",
        "            \"iframe\":'#cafe_main',\n",
        "            \"page_bar\":\"#main-area > div.prev-next\"\n",
        "        }\n",
        "        super().__init__(css_selectors, channel_name_to_url)\n",
        "        self.visit_thres = visit_thres\n",
        "        self.likes_thres = likes_thres\n",
        "        self.crawl_date_diff = crawl_date_diff if crawl_date_diff is None else datetime.datetime.today() - datetime.timedelta(days=crawl_date_diff)\n",
        "\n",
        "    def run_crawler(self, dir=None):\n",
        "        thread_list = []\n",
        "        for channel_name, channel_url in self.name_to_url.items():\n",
        "            if dir is not None:\n",
        "                channel_name = dir\n",
        "\n",
        "            os.makedirs(channel_name, exist_ok=True)\n",
        "            try:\n",
        "                self.crawling(channel_name, channel_url)\n",
        "            except NoSuchElementException:\n",
        "                print(\"페이지 수가 1000을 넘지 않습니다.\")\n",
        "            self.file_compressing(channel_name)\n",
        "\n",
        "            print(\"number of files : \", len(os.listdir(f\"{channel_name}\")))\n",
        "            print(\"=\"*50, end=\"\\n\\n\")\n",
        "\n",
        "        self.driver.close()\n",
        "\n",
        "    def download_image_and_caption(self, folder_name, file_name):\n",
        "        images = self.driver.find_elements_by_css_selector(self.css_selectors[\"image\"])\n",
        "        for img_idx, img in enumerate(images):\n",
        "            time.sleep(1)\n",
        "            file_full_name = f\"{file_name}_img{img_idx}\"\n",
        "            try:\n",
        "                with open(f\"{folder_name}/{file_full_name}.txt\", \"w\") as f:\n",
        "                    title_text = self.driver.find_element_by_css_selector(self.css_selectors[\"article_inner_title\"]).text\n",
        "                    f.write(title_text)\n",
        "\n",
        "                imgUrl = img.get_attribute(\"src\")\n",
        "                urllib.request.urlretrieve(imgUrl, f\"{folder_name}/{file_full_name}.png\")\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "    def crawling(self, channel_name, page_url):\n",
        "        print(f\"Start {channel_name} image crawling...\", end=\"\") \n",
        "        start_time = time.time()     \n",
        "        for page_idx in range(1, 1000 + 1):\n",
        "            current_url = page_url.replace(\"changable_index\", str(page_idx))\n",
        "            self.move_to_page(current_url) # 크롤링하고자 하는 페이지로 들어감\n",
        "\n",
        "            for table_idx in range(1, 15 + 1): # 팬아트 게시물은 15개씩 table로 나온다.\n",
        "                # 기본 정보 추출\n",
        "                art_date = self.driver.find_element_by_css_selector(self.css_selectors[\"date\"].replace(\"changable_index\", str(table_idx))).text\n",
        "                title = self.driver.find_element_by_css_selector(self.css_selectors[\"title\"].replace(\"changable_index\", str(table_idx)))\n",
        "                visit = str2float(self.driver.find_element_by_css_selector(self.css_selectors[\"visit\"].replace(\"changable_index\", str(table_idx))).text)\n",
        "                likes = str2float(self.driver.find_element_by_css_selector(self.css_selectors[\"likes\"].replace(\"changable_index\", str(table_idx))).text)\n",
        "                \n",
        "                if len(art_date) <= 6:\n",
        "                    art_date = datetime.datetime.today()\n",
        "                else:\n",
        "                    year, month, day = map(int, art_date[:-1].split(\".\"))\n",
        "                    art_date = datetime.datetime(year, month, day)\n",
        "                    \n",
        "                if self.crawl_date_diff is not None:\n",
        "                    if art_date < self.crawl_date_diff:\n",
        "                        break\n",
        "\n",
        "                # 조회수와 좋아요 수로 필터링\n",
        "                if visit < self.visit_thres or likes < self.likes_thres:\n",
        "                    continue\n",
        "\n",
        "                self.move_to_page(title.get_attribute('href')) # 팬아트 게시물 내부로 들어간다.\n",
        "                self.download_image_and_caption(folder_name=channel_name, file_name=f\"{str(art_date).split(' ')[0]}{page_idx}\") # 이미지를 다운로드 한다.\n",
        "                self.move_to_page(current_url) # 뒤로가기\n",
        "\n",
        "            else: \n",
        "                continue\n",
        "\n",
        "            break\n",
        "        print(f\" : done in {time.time() - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWzQ2vdqW24W"
      },
      "outputs": [],
      "source": [
        "class YouTubeThumbNailCrawler(NoHeadDriver):\n",
        "    def __init__(self, channel_name_to_url, crawl_date_diff=None, visit_thres=0):\n",
        "        '''\n",
        "        channel_name_to_url -> dict\n",
        "            - url의 닉네임을 key로, url을 value로 하여 딕셔너리를 만들면 된다.\n",
        "            ex)    {\"gomem_fan_art\":\"https://www.-----\", ---}\n",
        "        \n",
        "        crawl_date_diff -> int\n",
        "            - 오늘로부터 몇 일까지 크롤링할지 지정\n",
        "            - default로 둘 시, 모든 게시물을 크롤링한다.\n",
        "            ex) 3으로 지정시 오늘로부터 3일전의 영상까지 크롤링한다.\n",
        "            주의) 유튜브의 날짜 표기가 혐이기 때문에 비정확할 수 있다.\n",
        "            \n",
        "        method run_crawler -> void\n",
        "            - dir 파라미터에 경로를 주면 해당 경로에 데이터를 저장한다.\n",
        "        \n",
        "        '''\n",
        "        css_selectors = {\n",
        "            \"thumbnail\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/ytd-thumbnail/a\",\n",
        "            \"title\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/div[1]/div[1]/h3/a\",\n",
        "            \"visit\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/div[1]/div[1]/div/div[1]/div[2]/span[1]\",\n",
        "            \"elapsed_date\":\"/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-section-list-renderer/div[2]/ytd-item-section-renderer/div[3]/ytd-grid-renderer/div[1]/ytd-grid-video-renderer[changable_index]/div[1]/div[1]/div[1]/div/div[1]/div[2]/span[2]\"\n",
        "        }\n",
        "        super().__init__(css_selectors, channel_name_to_url)\n",
        "        self.crawl_date_diff = crawl_date_diff if crawl_date_diff is None else datetime.datetime.today() - datetime.timedelta(days=crawl_date_diff)\n",
        "        self.visit_thres = float(visit_thres)\n",
        "\n",
        "    def run_crawler(self, dir=None):\n",
        "        for channel_name, channel_url in self.name_to_url.items():\n",
        "            if dir is not None:\n",
        "                channel_name = dir\n",
        "\n",
        "            print(f\"Start {channel_name} image crawling...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            os.makedirs(channel_name, exist_ok=True)\n",
        "            self.move_to_page(channel_url)\n",
        "            self.scrolling_to_bottom()\n",
        "            self.crawling(channel_name)\n",
        "            self.file_compressing(channel_name)\n",
        "\n",
        "            print(f\" : done in {time.time() - start_time}\")\n",
        "            print(\"number of files : \", len(os.listdir(f\"{channel_name}\")))\n",
        "            print(\"=\"*50, end=\"\\n\\n\")\n",
        "    \n",
        "        self.driver.close()\n",
        "\n",
        "    def crawling(self, channel_name):  \n",
        "        idx = 0\n",
        "        while True:\n",
        "            idx += 1\n",
        "            title_xpath = self.css_selectors[\"title\"]\n",
        "            image_xpath = self.css_selectors[\"thumbnail\"]\n",
        "            visit_xpath = self.css_selectors[\"visit\"]\n",
        "            elapsed_date_xpath = self.css_selectors[\"elapsed_date\"]\n",
        "            try:\n",
        "                title = self.driver.find_element_by_xpath(title_xpath.replace(\"changable_index\", str(idx))).text\n",
        "                image = self.driver.find_element_by_xpath(image_xpath.replace(\"changable_index\", str(idx)))\n",
        "                visit = self.driver.find_element_by_xpath(visit_xpath.replace(\"changable_index\", str(idx))).text\n",
        "                elapsed_date = self.driver.find_element_by_xpath(elapsed_date_xpath.replace(\"changable_index\", str(idx))).text\n",
        "                \n",
        "            except NoSuchElementException as e:\n",
        "                print(\"no such element exception:\", e)\n",
        "                break\n",
        "                \n",
        "            if \"#Shorts\" in title or \"#shorts\" in title: \n",
        "                continue\n",
        "            if visit_thres > self.calc_visit_count_to_row(visit):\n",
        "                continue\n",
        "            if self.crawl_date_diff is not None:\n",
        "                if self.calc_date_by_elapsed_date(elapsed_date) < self.crawl_date_diff:\n",
        "                    break\n",
        "\n",
        "            file_name = datetime.date.today().isoformat()\n",
        "            with open(f\"{channel_name}/{file_name}_img{idx}.txt\", \"w\") as f:\n",
        "                f.write(title)\n",
        "\n",
        "            time.sleep(1)\n",
        "            image_url = \"https://i.ytimg.com/vi/\"+image.get_attribute(\"href\").replace(\"https://www.youtube.com/watch?v=\",\"\")+\"/hqdefault.jpg\"      \n",
        "            urllib.request.urlretrieve(image_url, f\"{channel_name}/{file_name}_img{idx}.png\")\n",
        "                \n",
        "    def calc_date_by_elapsed_date(self, elapsed_date):\n",
        "        elapsed_date = elapsed_date[:-2]\n",
        "        unit_to_dict = {\n",
        "            \"초\":0,\n",
        "            \"분\":0,\n",
        "            \"시간\":0,\n",
        "            \"일\":1,\n",
        "            \"주\":7,\n",
        "            \"개월\":30,\n",
        "            \"년\":365\n",
        "        }\n",
        "        \n",
        "        for unit, day in unit_to_dict.items():\n",
        "            if unit in elapsed_date:\n",
        "                return datetime.datetime.today() - datetime.timedelta(days=int(elapsed_date.replace(unit, \"\"))*day)\n",
        "        raise \"unknown error occur!\"\n",
        "            \n",
        "    def calc_visit_count_to_row(self, visit):\n",
        "        visit = visit[4:-1]\n",
        "        unit_to_dict = {\n",
        "            \"천\":1000,\n",
        "            \"만\":10000,\n",
        "            \"억\":100000000\n",
        "        }\n",
        "        for unit, count in unit_to_dict.items():\n",
        "            if unit in visit:\n",
        "                return float(visit.replace(unit, \"\"))*count\n",
        "        return float(visit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9393d5e6"
      },
      "outputs": [],
      "source": [
        "visit_thres = 1000\n",
        "likes_thres = 100\n",
        "\n",
        "wakzoo_name_to_url = {\n",
        "    \"isedol_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=344%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"gomem_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=299%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"hyung_fan_art\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=59%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "    \"geumson\":\"https://cafe.naver.com/steamindiegame?iframe_url=/ArticleList.nhn%3Fsearch.clubid=27842958%26search.menuid=551%26search.boardtype=L%26search.totalCount=151%26search.cafeId=27842958%26search.page=changable_index\",\n",
        "}\n",
        "\n",
        "youtube_name_to_url = {\n",
        "    \"gosegu_youtube\":\"https://www.youtube.com/channel/UCV9WL7sW6_KjanYkUUaIDfQ/videos\",\n",
        "    \"lilpa_youtube\":\"https://www.youtube.com/channel/UC-oCJP9t47v7-DmsnmXV38Q/videos\",\n",
        "    \"viichan_youtube\":\"https://www.youtube.com/channel/UCs6EwgxKLY9GG4QNUrP5hoQ/videos\",\n",
        "    \"ine_youtube\":\"https://www.youtube.com/channel/UCroM00J2ahCN6k-0-oAiDxg/videos\",\n",
        "    \"jururu_youtube\":\"https://www.youtube.com/c/%EC%A3%BC%EB%A5%B4%EB%A5%B4/videos\",\n",
        "    \"jingberger_youtube\":\"https://www.youtube.com/c/%EC%A7%95%EB%B2%84%EA%B1%B0/videos\",\n",
        "    \"wakgoob_youtube\":\"https://www.youtube.com/user/woowakgood/videos\",\n",
        "    \"waktaverse\":\"https://www.youtube.com/c/welshcorgimessi/videos\",\n",
        "    \"gyeleug\":\"https://www.youtube.com/channel/UChCqDNXQddSr0ncjs_78duA/videos\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhM520GjW29G",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fan_art_crawler = WakZooFanArtCrawler(wakzoo_name_to_url, visit_thres, likes_thres)\n",
        "fan_art_crawler.run_crawler()\n",
        "\n",
        "youtube_thumbnail_crawler = YouTubeThumbNailCrawler(youtube_name_to_url)\n",
        "youtube_thumbnail_crawler.run_crawler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb-3nBd8CYW8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "52c85bda6943e4a5376ffaf941c3086396b5dd3cb7ed1203ba9708a9139b9788"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}